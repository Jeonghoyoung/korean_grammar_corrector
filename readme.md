# Language Model

언어 모델(Language Model, LM)은 문장의 확률을 예측하거나, 이전 단어들이 주어졌을 때 다음 단어를 예측하는 모델입니다. 이를 통해 주어진 문장이 얼마나 자연스럽고 유창한 표현인지 계산할 수 있습니다.

---

## 언어 모델의 학습 및 구성
언어 모델은 다양한 문장을 수집하고 단어 간 출현 빈도를 기반으로 확률을 계산하여 학습됩니다.

### 한국어의 특징
- **교착어 구조**: 한국어는 단어의 의미와 역할이 어순보다는 접사나 조사에 의해 결정됩니다.
- **어휘 희소성**: 단어 어미를 분리하지 않으면 어휘의 수가 기하급수적으로 증가하여 확률 계산이 어려워집니다.

---

## N-gram
N-gram은 일부 단어 조합의 출현 빈도만 계산하여 확률을 추정하는 방법입니다.

- **마르코프 가정**: 특정 상태의 확률은 직전 상태에만 의존한다는 가정.
- 훈련 데이터가 충분할 경우 보통 3-gram을 가장 많이 사용합니다.
- 스무딩과 백오프(backoff)를 통해 훈련 데이터에 없는 단어 조합의 확률을 보완할 수 있습니다.

---

## 언어 모델 평가 방법

### 퍼플렉서티 (Perplexity, PPL)
PPL은 문장의 확률값을 정규화한 지표로, 낮을수록 언어 모델의 성능이 좋음을 나타냅니다.
- n-gram의 n이 클수록 더 낮은 PPL을 보이는 경향이 있습니다.
- 모델 성능은 교차 엔트로피 손실 값을 기반으로 PPL로 평가합니다.

---

## Neural Network Language Model (NNLM)

NNLM은 n-gram 기반 모델의 한계를 극복하고 희소성을 해소하여 일반화 성능을 향상시킵니다.
- 단어 간 유사성을 학습하여 새로운 단어 조합에 유연하게 대응합니다.

---

# 기계 번역

## Seq2Seq

Seq2Seq는 **인코더(Encoder)**, **디코더(Decoder)**, **생성자(Generator)**로 구성된 구조입니다.

### 구조
1. **Encoder**: 소스 문장을 압축하여 컨텍스트 벡터(Context Vector)로 변환.
2. **Decoder**: 컨텍스트 벡터와 이전에 생성된 단어를 기반으로 다음 단어를 생성.
3. **Generator**: 디코더의 hidden state에서 softmax를 통해 단어별 확률분포를 계산.

### 한계
- 고정된 크기의 컨텍스트 벡터에 모든 정보를 압축 → 정보 손실 발생.
- RNN 기반으로 기울기 소실 문제가 존재.

---

## Attention

Attention은 Seq2Seq의 한계를 보완하는 기법으로, 입력 전체를 동등하게 참조하지 않고 필요한 정보를 선택적으로 강조합니다.

### 주요 과정
1. **Query, Key, Value** 기반으로 유사도를 계산하여 컨텍스트 벡터를 생성.
2. 디코더의 t시점 hidden state를 기반으로 인코더의 모든 hidden state와의 가중합 수행.
3. 컨텍스트 벡터와 디코더 hidden state를 결합하여 예측 수행.

### Dot-Product Attention
- 디코더 hidden state와 인코더 hidden state 간의 유사도를 내적으로 계산.
- 소프트맥스를 적용하여 확률 분포(Attention 가중치) 생성.
- 가중합으로 최종 어텐션 값을 계산해 예측에 활용.

---

이 README는 언어 모델과 기계 번역의 주요 개념을 요약한 자료입니다. 추가 개선 사항이나 문의는 자유롭게 요청해주세요.
