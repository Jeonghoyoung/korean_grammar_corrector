# Language Model

- 언어 모델(LM)은 문장의 확률을 나타내는 모델로 언어 모델을 통해 문장 자체의 출현 확률을 예측하거나, 이전 단어들이 주어졌을떄 다음 단어를 예측할 수 있으며, 결과적으로 주어진 문장이 얼마나 자연스럽고 유창한 표현인지 계산.

- 언어 모델을 학습하고 구성하기 위해 많은 문장을 수집하고 단어와 단어 사이의 출현 빈도를 세어 확률 계산.

### 한국어의 특징
	- 한국어는 대표적인 교착어로, 단어의 의미 또는 역할은 어순에 의해 결정되기 보다는 단어에 부착되는 어미와 같은 접사 또는 조사에 의해 결정된다.

	- 단어의 어순이 중요하지 않기 떄문에 단어와 단어 사이의 확률을 계산하는데 불리함.

	- 어미를 분리해주지 않으면 어휘의 수가 기하급수적으로 늘어나 희소성이 높아져 문제해결이 더 어려워질 수 있음.
************	

### N-gram

- 전체 단어를 조합하는 대신 일부 단어 조합의 출현 빈도만을 계산하여 확률을 추정하는 방법.

- 마르코프 가정: 특정 시점의 상태 확률은 단지 그 직전 상태에만 의존한다는 논리로, 앞서 출현한 모든 단어를 살펴볼 필요없이, 앞의 k개의 단어만 보고 다음 단어의 출현 확률을 구하는 것.

- 훈련 코퍼스의 양이 적을수록 n의 크기도 작아진다.

- 훈련 코퍼스의 양이 적당하다는 가정 하에 보통 3-gram을 가장 많이 사용한다.

- 문장 전체의 확률에 대해 마르코프 가정을 도입하여 해당 문장의 확률을 근하살 수 있으며, 훈련 코퍼스에서 보지 못한 문장에 대해서도 확률을 추정할 수 있다.

	- 마르코프 가정 : 특정 시점의 상태 확률은 단지 그 직전 상태에만 의존한다는 논리, 즉 앞서 출현한 모든 단어를 살표볼 필요 없이 앞의 k개의 단어만 보고 다음 단어의 출현 확률을 구하는 것.

#### 결론
- 훈련 코퍼스에 등장하지 않은 단어 시퀀스의 확률을 정확하게 알 수 없는 n-gram 방식을 마르코ㅓ프 가정을 통해 단어 조합에 필요한 조건을 간소화 하고, 나아가 스무딩과 백오프 방식을 통해서 남은 단점을 보완.
************

### 언어모델 평가 방법

#### 퍼블렉서티 (PPL)
- 정량적 평가
- PPL은 문장의 길이를 반영하여 확률값을 정규화한 값
- PPL을 이용하여 언어 모델에서 테스트 문장들의 점수를 구하고 이를 기반으로 언어 모델의 성능을 측정.
- 확률값이 높을수록 PPL은 작아진다 즉 PPL은 수치가 낮을수록 좋으며 보통 n-gram의 n이 클수록 더 낮은 PPL을 보여준다.
- MLE를 통해 파라미터 를 학습할때 , 교차 엔트로피를 통해 얻은 손실값에 exp를 취함으로써 PPL을 얻어 언어 모델의 성능을 나태낼수 있다.

### NNLM

- n-gram 기반 언어 모델은 간편하지만 훈련 데이터에서 보지 못한 단어의 조합에 취약하다
- n-gram 기반 언어 모델은 단어간 유사도를 알지 못함.
- NNLM은 훈련 코퍼스에서 보지 못했던 단어의 조합을 만나더라도, 비슷한 훈련 데이터로부터 배운것과 유사하게 대처할 수 있다.
- 희소성 해소를 통해 더 좋은 일반화 성능을 얻을 수 있다.
************

## 기계 번역

### Seq2Seq

- seq2seq는 크게 3개의 서브모듈인 인코더, 디코더, 생성자(generator)로 구성되어있다.
- seq2seq를 모델 구조를 활용하여 MLE를 수행해 주어진 데이터를 가장 잘 설명하는 파라미터 theta 를 찾는다.
- p(Y|X;theta)를 최대로 하는 모델 파라미터를 찾는 작업을 수행하며 파라미터에 대한 학습이 완료되면, 사후확률을 최대로 하는 Y를 찾아야 한다.

```
seq2seq 구조

Encoder
- 주어진 소스 문장인 여러 개의 벡터를 입력으로 받아 하나의 벡터로 정보를 압축한다.
	- 이때의 벡터를 컨텍스트 벡터라고 한다.

- P(z|X)를 모델링 하고 주어진 문장을 매니폴드를 따라 차원 축소하여 해당 도메인의 잠재 공간의 어떤 하나의 벡터(Context vector)으로 변환하는 작업
 
 
- 텍스트 분류 문제에서는 모든 특징이 필요하지 않으나 기계번역을 위한 문장 임베딩 벡터를 생성하려면 최대한 많은 정보를 간직 해야 한다.


Decoder
- 인코더의 결과인 문장 임베딩 벡터와 이전 time-step까지 번역하여 생성한 단어들에 기반하여 현재 time-step의 단어 생성.

	- 인코더로부터 문장을 압축한 컨텍스트 벡터를 바탕으로 문장을 생성한다.

- 디코더 또한 신경망 언어 모델에 속하므로 신경망 언어 모델과 같이 디코더 입력의 초깃값으로써 y0 에 BOS(SOS) 토큰을 입력으로 준다.


Generator
- 디코더에서 각 time-step별로 hidden state를 받아 softmax를 계산하여 각 타깃 언어의 단어별 확률값을 지닌 확률분포를 반환하는 작업을 수행하는 모듈

- 단어를 선택하는 문제이므로 cross entropy loss를 통해 최적화 가능.

- 주의할 점은 문장의 길이가 m일때 맨 마지막 반환되는 단어는 EOS토큰이 된다는 점이다.
	- 생성이 끝남을 의미.

```

- seq2seq 문제
	1. 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하여서, 정보손실이 발생.
	2. RNN의 고질적인 문제인 기울기 소실 문제가 존재.

### Attention

- seq2seq의 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정하기 위해 등장한 기법
- 미분가능한 key-value function
- 어텐션은 쿼리와 비슷한 값을 가진 키를 찾아 그 값을 얻는 과정.
- 디코더의 hidden state의 한계로 인한 부족한 정보를 직접 encoder에 조회하여 예측에 필요한 정보를 얻어오는 과정.
- 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중.

- **정보를 잘 얻어오기 위해 Query를 잘 만들어내는 과정을 학습.**

```
Attention(Q,K,V) = Attention Value

	- 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구합니다. 그리고 구해낸 이 유사도를 키와 맵핑되어있는 각각의 '값(Value)'에 반영 -> 유사도가 반영된 '값(Value)'을 모두 더해서 리턴 == 'Attention Value'

'''
	Q = Query : 현재 time-step의 디코더 셀에서의 은닉 상태 (decoder output)
	K = Keys : 각 time-step별 인코더 셀의 은닉 상태들 (encoder output)
	V = Values : 각 time-step별 인코더 셀의 은닉 상태들 (encoder output)
'''

```

- Dot-Product Attention 
	- 코사인 유사도와 유사함.
	- 미니배치내의 각 문장별 디코더의 현재 time-stap에 대한 인코더의 전체 time-stap의 weight(유사도)

	1. Attention Score
		- 현재 디코더의 시점 t에서 단어를 예측하기 위해, 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉상태와 얼마나 유사한지를 판단하는 스코어값

		- Dot-Product Attention 에서는 이 스코어 값을 구하기 위해 은닉상태 $s_t$를 전치하고 각 은닉 상태와 내적을 수행한다. 즉, 모든 어텐션 스코어 값은 스칼라이다.

	2. 소프트맥스 함수를 통해 어텐션 분포를 구한다.
		- 모든 은닉상태의 어텐션 스코어 모음값 $e^t$ 에 소프트맥스 함수를 적용하여 모든 값의 합이 1이되는 확률 분포를 얻어 낼 수 있으며, 이를 어텐션 분포라 하고 각각의 값은 어텐션 가중치라고 한다.

	
	3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값을 구한다.
		- 각 인코더의 은닉 상태와 어텐션 가중치값들을 곱하고, 최종적으로 모두 더한다. 즉, 가중합(Weighted Sum)을 진행.
		- 어텐션 함수의 최종값인 어텐션 값은 종종 인코더의 문맥을 포함하고 있다고 하여 컨텍스트 벡터라고도 불림.

	4. 어텐션 값과 디코더의 t시점의 은닉상태를 연결.

		- 어텐션 값이 구해지면 어텐션 매커니즘은 어텐션 값을 은닉상태와 결합하여 하나의 벡터로 만드는 작업을 수행.
		- 이 결합값을 예측연산의 입력으로 사용하므로서 인코더로부터 얻은 정보를 활용하여 좀더 예측을 잘 할 수 있게됨

	5. 출력층 연산이 입력이 되는 $s_t^~$ 를 계산한다.
		- 어텐션 값과 은닉상태를 결합한 값을 가중치 행렬과 곱한 후 하이퍼볼릭탄젠트 함수를 지나도록 하여 출력층 연산을 위한 새로운 벡터를 얻는다.
	

	6. $s_t^~$ 를 출력층의 입력으로 사용하여 예측벡터를 얻는다.





























a