{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# keras_transformer Module\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "from keras_pos_embd import TrigPosEmbedding\n",
    "from keras_embed_sim import EmbeddingRet, EmbeddingSim\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구조\n",
    "1. text vectorize\n",
    "    - sentence -> sequence of number\n",
    "\n",
    "2. Embedding\n",
    "    - token embedding & positional embedding\n",
    "        - token embedding -> 단어 index 를 지정된 크기의 벡터로 맵핑\n",
    "\n",
    "        - positional embedding -> 각 단어의 위치 값에 대한 embedding\n",
    "\n",
    "\n",
    "3. Encoding Layer\n",
    "\n",
    "\n",
    "4. Decoding Layer\n",
    "\n",
    "\n",
    "\n",
    "** pip install keras_transformer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/71/zzhz1j712c90w725hts969l00000gn/T/ipykernel_42975/2368892654.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv('../../data/train/korean_corpus_train_20221201.shuf.csv', error_bad_lines=False, names=['src', 'tgt'])\n",
      "b'Skipping line 768178: expected 2 fields, saw 3\\n'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/train/korean_corpus_train_20221201.shuf.csv', error_bad_lines=False, names=['src', 'tgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>그런데도 왜 아힉도 수영모를 쓰고 계신지 궁금해요.</td>\n",
       "      <td>그런데도 왜 아직도 수영모를 쓰고 계신지 궁금해요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이에 따랇 이동제한 등 봉쇄조치는 이달 말까지 연장된다.</td>\n",
       "      <td>이에 따라 이동제한 등 봉쇄조치는 이달 말까지 연장된다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이러한 교류와 접초근 도시의 개방도와 국쩨화 정도를 로펴준다는 저메서 크 늬미를 가...</td>\n",
       "      <td>이러한 교류와 접촉은 도시의 개방도와 국제화 정도를 높여준다는 점에서 큰 의미를 가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>저희는 사고가 발생한 것을 증몽하는 영상이나 파일을 보내드릴 숬 있습니다.</td>\n",
       "      <td>저희는 사고가 발생한 것을 증명하는 영상이나 파일을 보내드릴 수 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이 세이버는 펴늬성과 스타일 미 텩씬적 층며네서 궁극쩌긴 시스테므로 디자인되얻씀니다.</td>\n",
       "      <td>이 세이버는 편의성과 스타일 및 혁신적 측면에서 궁극적인 시스템으로 디자인되었습니다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src  \\\n",
       "0                       그런데도 왜 아힉도 수영모를 쓰고 계신지 궁금해요.   \n",
       "1                    이에 따랇 이동제한 등 봉쇄조치는 이달 말까지 연장된다.   \n",
       "2  이러한 교류와 접초근 도시의 개방도와 국쩨화 정도를 로펴준다는 저메서 크 늬미를 가...   \n",
       "3          저희는 사고가 발생한 것을 증몽하는 영상이나 파일을 보내드릴 숬 있습니다.   \n",
       "4    이 세이버는 펴늬성과 스타일 미 텩씬적 층며네서 궁극쩌긴 시스테므로 디자인되얻씀니다.   \n",
       "\n",
       "                                                 tgt  \n",
       "0                       그런데도 왜 아직도 수영모를 쓰고 계신지 궁금해요.  \n",
       "1                    이에 따라 이동제한 등 봉쇄조치는 이달 말까지 연장된다.  \n",
       "2  이러한 교류와 접촉은 도시의 개방도와 국제화 정도를 높여준다는 점에서 큰 의미를 가...  \n",
       "3          저희는 사고가 발생한 것을 증명하는 영상이나 파일을 보내드릴 수 있습니다.  \n",
       "4    이 세이버는 편의성과 스타일 및 혁신적 측면에서 궁극적인 시스템으로 디자인되었습니다.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1619999it [01:10, 23006.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장당 평균 어절 길이 : 10.371815044330274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_length = 0\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    s = row['tgt']\n",
    "    sentence_length += len(s.split()) # 어절 개수 합산\n",
    "print(f'문장당 평균 어절 길이 : {sentence_length / len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(row['tgt'].split()) for i , row in df.iterrows()], bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1619999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    text = '[START] ' + text + ' [END]'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = df['src'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TextVectorization(standardize=text_preprocess, max_tokens=10)\n",
    "t.adapt(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 25), dtype=int64, numpy=\n",
       "array([[2, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [2, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 3],\n",
       "       [2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 0, 0, 0,\n",
       "        0, 0, 0]])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = t(df['src'][5:10])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = df['src'].apply(lambda x: text_preprocess(x))\n",
    "tgt = df['tgt'].apply(lambda x: text_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 [START] 그런데도 왜 아직도 수영모를 쓰고 계신지 궁금해요. [END]\n",
       "1              [START] 이에 따라 이동제한 등 봉쇄조치는 이달 말까지 연장된다. [END]\n",
       "2          [START] 이러한 교류와 접촉은 도시의 개방도와 국제화 정도를 높여준다는 점에서...\n",
       "3          [START] 저희는 사고가 발생한 것을 증명하는 영상이나 파일을 보내드릴 수 있습...\n",
       "4          [START] 이 세이버는 편의성과 스타일 및 혁신적 측면에서 궁극적인 시스템으로 ...\n",
       "                                 ...                        \n",
       "1619994                [START] 손끝에서 편안한 사용간을 주는 그립입니다. [END]\n",
       "1619995    [START] 앞으로 무슨 일이 일어날지 안내받지 못했던 콕스는 공연 중 \"일어나서...\n",
       "1619996             [START] 응, 그리고 그들 인생의 남자에게는 더욱 그래. [END]\n",
       "1619997    [START] 뛰어난 내구성을 가지고 안전 테스트를 거친 소재로 제작되어 비행 중에...\n",
       "1619998    [START] 초기 개척이 완성되어 삶이 안정화된 후에는 새로 이주하는 조선인들의 ...\n",
       "Name: tgt, Length: 1619999, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorizing(inputs, outputs, vocab_size, max_length):\n",
    "    def __full_stop_filter(text):\n",
    "        return re.sub(r'([?.!,])', r' \\1', str(text)).strip()\n",
    "\n",
    "    data= inputs + outputs\n",
    "    data = [i for d in data for i in d.split()]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "    text_vectorization = TextVectorization(\n",
    "        # standardize= __full_stop_filter,\n",
    "        max_tokens = vocab_size -1,\n",
    "        output_mode = 'int',\n",
    "        output_sequence_length = max_length + 1\n",
    "    )\n",
    "    text_vectorization.adapt(dataset)\n",
    "    return text_vectorization\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = src.tolist()[:5] + tgt.tolist()[:5]\n",
    "data = [i for d in data for i in d.split()]\n",
    "# data\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.preprocessing.text_vectorization.TextVectorization at 0x7f958e27fd00>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Vectorizing(src.tolist()[:5], tgt.tolist()[:5], 1000, 30)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.preprocessing.text_vectorization.TextVectorization at 0x7f958e27fd00>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoyoung/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/keras/engine/data_adapter.py:656: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x = np.asarray(x)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb 셀 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test \u001b[39m=\u001b[39m Vectorizing(src, tgt, vocab_size\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39m70\u001b[39;49m)\n",
      "\u001b[1;32m/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb 셀 17\u001b[0m in \u001b[0;36mVectorizing\u001b[0;34m(inputs, outputs, vocab_size, max_length)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb#Y106sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m data \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb#Y106sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m text_vectorization \u001b[39m=\u001b[39m TextVectorization(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb#Y106sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     standardize\u001b[39m=\u001b[39m __full_stop_filter,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb#Y106sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     max_tokens \u001b[39m=\u001b[39m vocab_size \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb#Y106sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     output_mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb#Y106sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     output_sequence_length \u001b[39m=\u001b[39m max_length \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb#Y106sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb#Y106sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m text_vectorization\u001b[39m.\u001b[39;49madapt(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb#Y106sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m text_vectorization\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/keras/layers/preprocessing/text_vectorization.py:428\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madapt\u001b[39m(\u001b[39mself\u001b[39m, data, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    382\u001b[0m   \u001b[39m\"\"\"Computes a vocabulary of string terms from tokens in a dataset.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \n\u001b[1;32m    384\u001b[0m \u001b[39m  Calling `adapt()` on a `TextVectorization` layer is an alternative to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[39m        argument is not supported with array inputs.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m   \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49madapt(data, batch_size\u001b[39m=\u001b[39;49mbatch_size, steps\u001b[39m=\u001b[39;49msteps)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/keras/engine/base_preprocessing_layer.py:238\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m    237\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_state()\n\u001b[0;32m--> 238\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mDataHandler(\n\u001b[1;32m    239\u001b[0m     data,\n\u001b[1;32m    240\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    241\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m    242\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    243\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[1;32m    244\u001b[0m     distribute\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    245\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapt_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_adapt_function()\n\u001b[1;32m    246\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/keras/engine/data_adapter.py:1151\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1150\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1152\u001b[0m     x,\n\u001b[1;32m   1153\u001b[0m     y,\n\u001b[1;32m   1154\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1155\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1156\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1157\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1158\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1159\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1160\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1161\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1162\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1163\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m   1165\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/keras/engine/data_adapter.py:664\u001b[0m, in \u001b[0;36mListsOfScalarsDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m   sample_weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(sample_weights)\n\u001b[1;32m    661\u001b[0m sample_weight_modes \u001b[39m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[1;32m    662\u001b[0m     sample_weights, sample_weight_modes)\n\u001b[0;32m--> 664\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_adapter \u001b[39m=\u001b[39m TensorLikeDataAdapter(\n\u001b[1;32m    665\u001b[0m     x,\n\u001b[1;32m    666\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    667\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weights,\n\u001b[1;32m    668\u001b[0m     sample_weight_modes\u001b[39m=\u001b[39;49msample_weight_modes,\n\u001b[1;32m    669\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    670\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    671\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/keras/engine/data_adapter.py:236\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    226\u001b[0m              x,\n\u001b[1;32m    227\u001b[0m              y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m              shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    234\u001b[0m              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    235\u001b[0m   \u001b[39msuper\u001b[39m(TensorLikeDataAdapter, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(x, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 236\u001b[0m   x, y, sample_weights \u001b[39m=\u001b[39m _process_tensorlike((x, y, sample_weights))\n\u001b[1;32m    237\u001b[0m   sample_weight_modes \u001b[39m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[1;32m    238\u001b[0m       sample_weights, sample_weight_modes)\n\u001b[1;32m    240\u001b[0m   \u001b[39m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/keras/engine/data_adapter.py:1044\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[39mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[1;32m   1042\u001b[0m   \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m-> 1044\u001b[0m inputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_convert_single_tensor, inputs)\n\u001b[1;32m   1045\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/keras/engine/data_adapter.py:1039\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_single_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, np\u001b[39m.\u001b[39mfloating):\n\u001b[1;32m   1038\u001b[0m     dtype \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mfloatx()\n\u001b[0;32m-> 1039\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(x, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m   1040\u001b[0m \u001b[39melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[1;32m   1041\u001b[0m   \u001b[39mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "test = Vectorizing(src, tgt, vocab_size=10000, max_length=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Embedding(vocab_size, embed_dim, hidden_dim):\n",
    "    inputs = keras.Input(shape=(1,), name='input_data', dtype=tf.string)\n",
    "    token_emb = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "    maxlen = tf.shape(inputs)[-1]\n",
    "    positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "    positions = layers.Embedding(vocab_size, embed_dim)(positions)\n",
    "    result = token_emb + positions\n",
    "    return tf.keras.Model(inputs=input_raw_text, outputs=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Inputs to a layer should be tensors. Got: <START>그런데도 왜 아힉도 수영모를 쓰고 계신지 궁금해요.<END><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb 셀 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hoyoung/Desktop/pycharm_work/korean_grammar_corrector/bin/keras_model_v2/textvectorization.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Embedding(\u001b[39m2000\u001b[39;49m, \u001b[39m16\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m70\u001b[39;49m)(src[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ko_grammer_checker/lib/python3.8/site-packages/keras/engine/input_spec.py:197\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m inputs:\n\u001b[1;32m    192\u001b[0m   \u001b[39m# Having a shape/dtype is the only commonality of the various tensor-like\u001b[39;00m\n\u001b[1;32m    193\u001b[0m   \u001b[39m# objects that may be passed. The most common kind of invalid type we are\u001b[39;00m\n\u001b[1;32m    194\u001b[0m   \u001b[39m# guarding for is a Layer instance (Functional API), which does not\u001b[39;00m\n\u001b[1;32m    195\u001b[0m   \u001b[39m# have a `shape` attribute.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m'\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 197\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInputs to a layer should be tensors. Got: \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    199\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(input_spec):\n\u001b[1;32m    200\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLayer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m expects \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(input_spec)\u001b[39m}\u001b[39;00m\u001b[39m input(s),\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    201\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m but it received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(inputs)\u001b[39m}\u001b[39;00m\u001b[39m input tensors. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    202\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInputs received: \u001b[39m\u001b[39m{\u001b[39;00minputs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Inputs to a layer should be tensors. Got: <START>그런데도 왜 아힉도 수영모를 쓰고 계신지 궁금해요.<END><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>"
     ]
    }
   ],
   "source": [
    "Embedding(2000, 16, 32, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(embed_dim, hidden_dim, num_heads, name='encoder_layer'):\n",
    "    encoder_inputs = keras.Input(shape=(None, embed_dim), name=\"inputs\")\n",
    "    # 패딩마스크\n",
    "    # padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    attention = MultiHeadAttention(head_num=num_heads, activation='relu'\n",
    "    , history_only=True,trainable=True)(encoder_inputs)\n",
    "    attention = layers.Dropout(0.1)(attention)\n",
    "    attention = layers.Normalization()(encoder_inputs + attention)\n",
    "\n",
    "    outputs = layers.Dense(hidden_dim, activation='relu')(attention)\n",
    "    outputs = layers.Dense(embed_dim)(outputs)\n",
    "\n",
    "    outputs = layers.Dropout(0.1)(outputs)\n",
    "    outputs = layers.Normalization()(attention + outputs)\n",
    "\n",
    "    return keras.Model(\n",
    "        inputs = encoder_inputs,\n",
    "        outputs = outputs,\n",
    "        name = name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoder(embed_dim, hidden_dim, num_heads, name='decoder_layer'):\n",
    "    decoder_inputs = keras.Input(shape=(None, embed_dim), name=\"inputs\")\n",
    "    encoder_outputs = keras.Input(shape=(None, embed_dim), name=\"encoder_outputs\")\n",
    "\n",
    "    attention1 =  MultiHeadAttention(head_num=num_heads, activation='relu'\n",
    "    , history_only=True,trainable=True)(decoder_inputs)\n",
    "    attention1 = layers.Normalization()(decoder_inputs + attention1)\n",
    "\n",
    "    attention2 = MultiHeadAttention(name='multyhead-attention', head_num=num_heads, activation='relu'\n",
    "    , history_only=True,trainable=True)([decoder_inputs, encoder_outputs, encoder_outputs])\n",
    "\n",
    "    attention2 = layers.Dropout(0.1)(attention2)\n",
    "    attention2 = layers.Normalization()(attention1 + attention2)\n",
    "\n",
    "    outputs = layers.Dense(hidden_dim, activation='relu')(attention2)\n",
    "    outputs = layers.Dense(embed_dim)(outputs)\n",
    "\n",
    "    outputs = layers.Dropout(0.1)(outputs)\n",
    "    outputs = layers.Normalization()(attention2 + outputs)\n",
    "\n",
    "    return keras.Model(\n",
    "        inputs = [decoder_inputs, encoder_outputs],\n",
    "        outputs = outputs,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoders(vocab_size, max_length, num_layers, embed_dim, hidden_dim, num_heads, dropout):\n",
    "    inputs = keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    embedding = Embedding(vocab_size, embed_dim, hidden_dim, max_length)(inputs)\n",
    "    outputs = layers.Dropout(dropout)(embedding)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = Encoder(embed_dim, hidden_dim, num_heads, name = f'encoder_layer_{i+1}')(outputs)\n",
    "    \n",
    "\n",
    "    return keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs = outputs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoders(vocab_size, max_length, num_layers, embed_dim, hidden_dim, num_heads, dropout):\n",
    "    inputs = keras.Input(shape=(None,), name=\"inputs\")\n",
    "    encoder_outputs = keras.Input(shape=(None, embed_dim), name=\"encoder_outputs\")\n",
    "\n",
    "    embedding = Embedding(vocab_size, embed_dim, hidden_dim, max_length)(inputs)\n",
    "    outputs = layers.Dropout(dropout)(embedding)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = Decoder(embed_dim, hidden_dim, num_heads, name=f'decoder_layer_{i+1}')([outputs, encoder_outputs])\n",
    "    \n",
    "    return keras.Model(\n",
    "        inputs=[inputs, encoder_outputs],\n",
    "        outputs = outputs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer(vocab_size, embed_dim, hidden_dim, encoder_num, decoder_num, num_heads, dropout, max_length, name='transformer'):\n",
    "    inputs = keras.Input(shape=(None,), name=\"inputs\")\n",
    "    decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
    "\n",
    "    encoder_outputs = get_encoders(vocab_size, max_length, encoder_num, embed_dim, hidden_dim, num_heads, dropout)(inputs)\n",
    "\n",
    "    decoder_outputs = get_decoders(vocab_size, max_length, decoder_num, embed_dim, hidden_dim, num_heads, dropout)([inputs, encoder_outputs])\n",
    "\n",
    "    outputs = layers.Dense(vocab_size, name='outputs')(decoder_outputs)\n",
    "\n",
    "    return keras.Model(inputs = [inputs, decoder_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = get_transformer(vocab_size=8000,\n",
    "embed_dim=128,\n",
    "hidden_dim=256,\n",
    "encoder_num=4,\n",
    "decoder_num=4,\n",
    "num_heads=4,\n",
    "dropout=0.1,\n",
    "max_length=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputs (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " model_9 (Functional)           (None, 36, 128)      2577928     ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " model_11 (Functional)          (None, 36, 128)      2843148     ['inputs[0][0]',                 \n",
      "                                                                  'model_9[0][0]']                \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 36, 8000)     1032000     ['model_11[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,453,076\n",
      "Trainable params: 6,447,936\n",
      "Non-trainable params: 5,140\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ko_grammer_checker')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a836cf7ca6b519c98c7fd6811d6f732b6df88dee37c990b7a838f21f96c41064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
