{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7f7c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import utils.tensorflow_preprocess as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3490bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/colloquial_correct_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa5ce65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>네, 언제든지 편하실 때 체크아우타시면 도와드릴게요.</td>\n",
       "      <td>네, 언제든지 편하실 때 체크아웃하시면 도와드릴게요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>성장하는 재판매 사업짜 그루베 고갱니믈 초대하고 십씀니다.</td>\n",
       "      <td>성장하는 재판매 사업자 그룹에 고객님을 초대하고 싶습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>안녕하세요, 예야카려고 전화를 드려써요.</td>\n",
       "      <td>안녕하세요, 예약하려고 전화를 드렸어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>손니미 완는데 방이 업쓰며 너떠캐요?</td>\n",
       "      <td>손님이 왔는데 방이 없으면 어떡해요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>아니요, 시간 낭비예요, 다시느 녀기에 아 놀 꺼예요.</td>\n",
       "      <td>아니요, 시간 낭비예요, 다시는 여기에 안 올 거예요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>어떤 다른 서류가 필요하신지 닶시 말쐼해 주시겠습니까?</td>\n",
       "      <td>어떤 다른 서류가 필요하신지 다시 말씀해 주시겠습니까?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>비행기에서 감배를 피울 생각읃 없었어요.</td>\n",
       "      <td>비행기에서 담배를 피울 생각은 없었어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>전 오래 씂 수 깄는 겍 필요해요.</td>\n",
       "      <td>전 오래 쓸 수 있는 게 필요해요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>털앨 궁화하고 앉았을 땑 반짝이고 푹신한 느낌을 줍니다.</td>\n",
       "      <td>털을 강화하고 앉았을 때 반짝이고 푹신한 느낌을 줍니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>그렇지 않은 경우, 원하는 시간을 제안해 주십시오.</td>\n",
       "      <td>그렇지 않은 경우, 원하는 시간을 제안해 주십시오.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    src                               tgt\n",
       "0         네, 언제든지 편하실 때 체크아우타시면 도와드릴게요.     네, 언제든지 편하실 때 체크아웃하시면 도와드릴게요.\n",
       "1      성장하는 재판매 사업짜 그루베 고갱니믈 초대하고 십씀니다.  성장하는 재판매 사업자 그룹에 고객님을 초대하고 싶습니다.\n",
       "2                안녕하세요, 예야카려고 전화를 드려써요.            안녕하세요, 예약하려고 전화를 드렸어요.\n",
       "3                  손니미 완는데 방이 업쓰며 너떠캐요?              손님이 왔는데 방이 없으면 어떡해요?\n",
       "4        아니요, 시간 낭비예요, 다시느 녀기에 아 놀 꺼예요.    아니요, 시간 낭비예요, 다시는 여기에 안 올 거예요.\n",
       "...                                 ...                               ...\n",
       "49995    어떤 다른 서류가 필요하신지 닶시 말쐼해 주시겠습니까?    어떤 다른 서류가 필요하신지 다시 말씀해 주시겠습니까?\n",
       "49996            비행기에서 감배를 피울 생각읃 없었어요.            비행기에서 담배를 피울 생각은 없었어요.\n",
       "49997               전 오래 씂 수 깄는 겍 필요해요.               전 오래 쓸 수 있는 게 필요해요.\n",
       "49998   털앨 궁화하고 앉았을 땑 반짝이고 푹신한 느낌을 줍니다.   털을 강화하고 앉았을 때 반짝이고 푹신한 느낌을 줍니다.\n",
       "49999      그렇지 않은 경우, 원하는 시간을 제안해 주십시오.      그렇지 않은 경우, 원하는 시간을 제안해 주십시오.\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f81b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_src'] = df['src'].apply(lambda x: tp.full_stop_filter(x))\n",
    "df['n_tgt'] = df['tgt'].apply(lambda x: tp.full_stop_filter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcd03dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = tp.tokenize_and_filter(df['n_src'], df['n_tgt'], max_length=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1e19119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (50000, 24)\n",
      "Output shape: (50000, 24)\n"
     ]
    }
   ],
   "source": [
    "print(f'Input shape: {inputs.shape}')\n",
    "print(f'Output shape: {outputs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f2f9d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 14:59:17.648714: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tf_dataset = tp.create_train_dataset(inputs, outputs, batch_size=64, buffer_size=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d6ae565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'inputs': TensorSpec(shape=(None, 24), dtype=tf.int32, name=None), 'dec_inputs': TensorSpec(shape=(None, 23), dtype=tf.int32, name=None)}, {'outputs': TensorSpec(shape=(None, 23), dtype=tf.int32, name=None)})>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49afd239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8226,   23,    2, ...,    0,    0,    0],\n",
       "       [8226, 5309,    5, ...,    0,    0,    0],\n",
       "       [8226,   55,    2, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [8226,  303,  919, ...,    0,    0,    0],\n",
       "       [5591, 8002, 1213, ...,  368,    1, 8227],\n",
       "       [8226, 3214, 1181, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b54612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    '''\n",
    "    Positional Embedding 의 pos / 10000 ** (2i / d_model) 부분 산출 함수\n",
    "\n",
    "    PE(pos, 2i) = sin(pos / 10000 ** (2i / d_model))\n",
    "    PE(pos, 2i + 1) = cos(pos / 10000 ** (2i / d_model))\n",
    "    '''\n",
    "    angle_rates = 1 / np.power(10000, (2 * i // 2) / np.float32(d_model))\n",
    "    # shape = (1, d_model)\n",
    "    # pos shape :\n",
    "    # angle_rates shape = (1, d_model)\n",
    "    # return shape = 행렬곱 (pos, 1), (1, d_model) =  (pos, d_model)\n",
    "    return np.matmul(pos, angle_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95beb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)  # shape : (position, d_model)\n",
    "    # 오른쪽으로 짝수번째 인덱스는 sin 함수를 적용\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # 오른쪽으로 홀수번째 인덱스는 cos 함수를 적용\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]  # pos_encoding shape : (1, position, d_model)\n",
    "    # 왜 shape에 1을 추가해주냐면, batch_size 만큼 학습하기 위함임\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4163731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, input_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model  # 하나의 단어가 d_model의 차원으로 인코딩 됨\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        # vocab_size는 tokenizer 내부 vocab.txt의 사이즈\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)  # 포지셔널 인코딩\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)  # 드롭아웃 설정\n",
    "\n",
    "    def __call__(self, x, training):\n",
    "        # 최초 x의 shape = (batch_size, seq_len)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        out = self.embedding(x)  # shape : (batch_size, input_seq_len, d_model)\n",
    "        out = out * tf.math.sqrt(\n",
    "            tf.cast(self.d_model, tf.float32))  # x에 sqrt(d_model) 만큼을 곱해주냐면, 임베딩 벡터보다 포지셔널 인코딩 임베딩 벡터의 영향력을 줄이기 위해서임\n",
    "        # 포지셔널 인코딩은 순서만을 의미하기 때문에 임베딩 벡터보다 영향력이 적어야 이치에 맞음\n",
    "        out = out + self.pos_encoding[:, :seq_len, :]\n",
    "        out = self.dropout(out, training=training)\n",
    "\n",
    "        return out  # shape : (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2d2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    # q shape : (batch_size, seq_len, d_model)\n",
    "    # k shape : (batch_size, seq_len, d_model)\n",
    "    # v shape : (batch_size, seq_len, d_model)\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b = True)\n",
    "    #matmul_qk shape : (batch_size, seq_len, seq_len)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # scaled_attetion_logits shape : (batch_size, seq_len, seq_len)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits = scaled_attention_logits + (mask * -1e9)\n",
    "\n",
    "    softmax = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    # softmax shape : (batch_size, seq_len, seq_len)\n",
    "\n",
    "    output = tf.matmul(softmax, v)\n",
    "\n",
    "    # output(attention_value) shape : (batch_size, seq_len, d_model)\n",
    "    # 즉 처음 입력 차원인 (batch_size, seq_len, d_model) 차원을 아웃풋으로 반환\n",
    "    # 인풋과 아웃풋의 사이즈가 동일하다.\n",
    "\n",
    "    # scaled_dot_product_attention 의 결과는 단어들 간의 연관성을 학습.\n",
    "    return output, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c5e785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    # 멀티 헤드 어텐션은 전체 어텐션을 분리하여 병렬적으로 어텐션을 수행하는 기법.\n",
    "    # 이렇게 하는 이유는, 깊은 차원을 한번에 어텐션을 수행하는 것보다, 병렬로 각각 수행하는 것이 더 심도있는 언어들간의 관계를 학습할 수 있기 때문.\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0,2,1,3])\n",
    "\n",
    "    def __call__(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "\n",
    "        attention_weights, softmax = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(attention_weights, perm=[0,2,1,3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3f90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pointwise_FeedForward_Network(tf.keras.layers.Layer):\n",
    "    # Pointwise_FeedForward_Network 에서는 인코더의 출력에서 512개의 차원이 2048차원까지 확장되고, 다시 512개의 차원으로 압축된다.\n",
    "    def __init__(self, d_model, dff):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "\n",
    "        self.middle = tf.keras.layers.Dense(dff, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        middle = self.middle(x) # middle shape : (batch_size, seq_len, dff)\n",
    "        out = self.out(middle) # out shape : (batch_size, seq_len, d_model)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ad318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ko_grammer_checker] *",
   "language": "python",
   "name": "conda-env-ko_grammer_checker-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
